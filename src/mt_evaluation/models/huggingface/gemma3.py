# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: CC-BY-NC-4.0

from typing import List, Dict, Union
from tqdm import tqdm
import logging

import torch
from transformers import (
    AutoTokenizer,
    AutoProcessor,
    Gemma3ForCausalLM,
    Gemma3ForConditionalGeneration,
)

from mt_evaluation.core import FewShots
from mt_evaluation.models.base import Model

logger = logging.getLogger(__name__)


class Gemma3(Model):

    text2text_models = {"google/gemma-3-1b-it"}
    imagetext2text_models = {
        "google/gemma-3-27b-it",
        "google/gemma-3-12b-it",
        "google/gemma-3-4b-it",
    }

    def __init__(
        self, model_id: str, max_batch_size: int = 4, max_cache_len: int = 1024
    ):
        super().__init__(model_id)

        if model_id in self.text2text_models:
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            self.model = Gemma3ForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                attn_implementation="flash_attention_2",
            ).eval()
        elif model_id in self.imagetext2text_models:
            self.tokenizer = AutoProcessor.from_pretrained(model_id)
            self.model = Gemma3ForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                attn_implementation="flash_attention_2",
            ).eval()
        else:
            raise ValueError(f"Unknown Gemma model: {model_id}")

        self.tokenizer.padding_side = "left"

        """
        # To use static k-v cache as in https://huggingface.co/docs/transformers/v4.44.1/en/llm_optims?static-kv=advanced+usage%3A+control+Static+Cache
        self.model.forward = torch.compile(
            self.model.forward, mode="reduce-overhead", fullgraph=True
        )

        config = AutoConfig.from_pretrained(model_id)
        for key, value in vars(config.text_config).items():
            setattr(config, key, value)

        self.past_key_values = StaticCache(
            config=config,
            max_batch_size=max_batch_size,
            max_cache_len=max_cache_len,
            device=self.model.device,
            dtype=self.model.dtype,
        )
        """

    def __call__(
        self,
        system_prompts: List[str],
        user_prompts: List[str],
        few_shots: List[FewShots],
        batch_size: int = 4,
        max_new_tokens: int = 512,
        **kwargs,
    ) -> List[str]:
        """
        Generate the answer to the given prompts

        Args:
            system_prompts (List[str]): List of system_prompts
            user_prompts (List[str]): List of user prompts, to be associated to system_prompts
            few_shots (List[Dict]): List of examples paired with the desired model answers to use for in-context learning
            batch_size (int): Number of prompts to process in one batch
            max_new_tokens (int): Maximum number of tokens to be generated by the model

        Returns:
            List[str]: List of responses, one per prompt
        """

        responses = []
        # Process sources in batches
        for i in tqdm(
            range(0, len(user_prompts), batch_size),
            desc=f"Running inference with {self.name}",
            total=(len(user_prompts) + batch_size - 1) // batch_size,
        ):
            batch_system_prompts = system_prompts[i : i + batch_size]
            batch_user_prompts = user_prompts[i : i + batch_size]
            batch_few_shots = few_shots[i : i + batch_size]

            messages = []
            for system_prompt, user_prompt, sample_few_shots in zip(
                batch_system_prompts, batch_user_prompts, batch_few_shots
            ):
                sample_message = [
                    {
                        "role": "system",
                        "content": [
                            {"type": "text", "text": system_prompt},
                        ],
                    },
                ]

                for shot_user_prompt, shot_response in zip(
                    sample_few_shots.user_prompts, sample_few_shots.assistant_responses
                ):
                    sample_message += [
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": shot_user_prompt},
                            ],
                        },
                        {
                            "role": "assistant",
                            "content": [
                                {"type": "text", "text": shot_response},
                            ],
                        },
                    ]

                sample_message += [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": user_prompt},
                        ],
                    },
                ]

                messages.append(sample_message)

            logger.debug(f"Example message: {messages[0]}")

            inputs = self.tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
                padding=True,
            ).to(self.model.device)

            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    # past_key_values=self.past_key_values,
                    # cache_implementation=None,
                )
            # self.past_key_values.reset()

            decoded_outputs = self.tokenizer.batch_decode(
                outputs[:, inputs["input_ids"].shape[1] :],
                skip_special_tokens=True,
            )

            responses += decoded_outputs

        return responses
