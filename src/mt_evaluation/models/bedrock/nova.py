# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: CC-BY-NC-4.0

from typing import List, Dict, Union, Optional
import logging
import json
import asyncio

from mt_evaluation.core import FewShots
from mt_evaluation.models.bedrock import BedrockResponse
from mt_evaluation.models.bedrock.base import BedrockModel


class Nova(BedrockModel):

    price_per_1000_input_tokens = None
    price_per_1000_output_tokens = None

    async def single_call(
        self,
        messages: List[Dict],
        system_message: List[Dict],
        inf_params: Dict,
    ):
        request_body = {
            "schemaVersion": "messages-v1",
            "messages": messages,
            "system": system_message,
            "inferenceConfig": inf_params,
        }

        invoke_model_inputs = {
            "body": json.dumps(request_body),
            "modelId": self.name,
            "accept": "application/json",
            "contentType": "application/json",
        }

        return await self.invoke_model_and_read_response(invoke_model_inputs)

    async def concurrent_call(
        self,
        messages: List[List[Dict]],
        system_messages: List[List[Dict]],
        inf_params,
    ):

        tasks = []
        for sample_msg, sample_sys_msg in zip(messages, system_messages):
            tasks.append(self.single_call(sample_msg, sample_sys_msg, inf_params))

        responses = await asyncio.gather(*tasks)

        return responses

    async def _async_call(
        self,
        system_prompts: List[str],
        user_prompts: List[str],
        few_shots: List[FewShots],
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        **kwargs,
    ) -> List[BedrockResponse]:
        """
        Generate the answer to the given prompts

        Args:
            system_prompts (List[str]): List of system_prompts
            user_prompts (List[str]): List of user prompts, to be associated to system_prompts
            few_shots (List[Dict]): List of examples paired with the desired model answers to use for in-context learning
            batch_size (int): Number of prompts to process in one batch
            max_new_tokens (int): Maximum number of tokens to be generated by the model

        Returns:
            List[BedrockResponse]: List of responses, one per prompt
        """

        if len(user_prompts) != len(few_shots) != len(system_prompts):
            raise ValueError(
                f"Number of system prompts, user prompts, and few-shots must match! ({len(system_prompts)}, {len(user_prompts)}, {len(few_shots)})"
            )

        system_messages, messages = [], []
        for system_prompt, user_prompt, sample_few_shots in zip(
            system_prompts, user_prompts, few_shots
        ):
            sample_message = []

            for shot_user_prompt, shot_response in zip(
                sample_few_shots.user_prompts, sample_few_shots.assistant_responses
            ):
                sample_message += [
                    {
                        "role": "user",
                        "content": [{"text": shot_user_prompt}],
                    },
                    {
                        "role": "assistant",
                        "content": [{"text": shot_response}],
                    },
                ]

            sample_message += [
                {
                    "role": "user",
                    "content": [{"text": user_prompt}],
                },
            ]

            system_messages.append([{"text": system_prompt}])
            messages.append(sample_message)

        inf_params = {
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "topP": top_p,
        }

        if len(messages) == len(system_messages) == 1:
            messages = messages[0]
            system_messages = system_messages[0]

            responses = [await self.single_call(messages, system_messages, inf_params)]
        else:
            responses = await self.concurrent_call(
                messages, system_messages, inf_params
            )

        responses = [
            BedrockResponse(
                num_input_tokens=response["usage"]["inputTokens"],
                num_output_tokens=response["usage"]["outputTokens"],
                response=response["output"]["message"]["content"][0]["text"],
                stop_reason=response["stopReason"],
                cost=self.compute_cost(
                    response["usage"]["inputTokens"], response["usage"]["outputTokens"]
                ),
            )
            for response in responses
        ]

        return responses


class NovaPro(Nova):

    price_per_1000_input_tokens = 0.0008 * 0.3
    price_per_1000_output_tokens = 0.0032 * 0.3
