# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: CC-BY-NC-4.0

from typing import List, Dict
from tqdm import tqdm

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from mt_evaluation.core import FewShots
from mt_evaluation.models.base import Model


class Llama3(Model):
    """
    Llama-based auto-evaluation using AWS SageMaker JumpStart.
    """

    def __init__(
        self,
        model_id: str,
    ):
        super().__init__(model_id)

        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id, torch_dtype=torch.bfloat16, device_map="auto"
        )

        self.tokenizer.padding_side = "left"
        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        self.tokenizer.pad_token = self.tokenizer.eos_token

    def __call__(
        self,
        system_prompts: List[str],
        user_prompts: List[str],
        few_shots: List[FewShots],
        batch_size: int = 4,
        max_new_tokens: int = 512,
        **kwargs,
    ) -> List[str]:
        """
        Generate the answer to the given prompts

        Args:
            system_prompts (List[str]): List of system_prompts
            user_prompts (List[str]): List of user prompts, to be associated to system_prompts
            few_shots (List[Dict]): List of examples paired with the desired model answers to use for in-context learning
            batch_size (int): Number of prompts to process in one batch
            max_new_tokens (int): Maximum number of tokens to be generated by the model

        Returns:
            List[str]: List of responses, one per prompt
        """
        responses = []
        # Process sources in batches
        for i in tqdm(
            range(0, len(user_prompts), batch_size),
            desc=f"Running inference with {self.name}",
            total=(len(user_prompts) + batch_size - 1) // batch_size,
        ):
            batch_system_prompts = system_prompts[i : i + batch_size]
            batch_user_prompts = user_prompts[i : i + batch_size]
            batch_few_shots = few_shots[i : i + batch_size]

            messages = []
            for system_prompt, user_prompt, sample_few_shots in zip(
                batch_system_prompts, batch_user_prompts, batch_few_shots
            ):
                sample_message = [
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                ]

                for shot_user_prompt, shot_response in zip(
                    sample_few_shots.user_prompts, sample_few_shots.assistant_responses
                ):
                    sample_message += [
                        {
                            "role": "user",
                            "content": shot_user_prompt,
                        },
                        {
                            "role": "assistant",
                            "content": shot_response,
                        },
                    ]

                sample_message += [
                    {
                        "role": "user",
                        "content": user_prompt,
                    },
                ]

                messages.append(sample_message)

            inputs = self.tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
                padding=True,
            ).to(self.model.device)

            terminators = [
                self.tokenizer.eos_token_id,
                self.tokenizer.convert_tokens_to_ids("<|eot_id|>"),
            ]

            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    eos_token_id=terminators,
                    do_sample=False,
                )

            decoded_outputs = self.tokenizer.batch_decode(
                outputs[:, inputs["input_ids"].shape[1] :],
                skip_special_tokens=True,
            )

            responses += decoded_outputs

        return responses
