# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: CC-BY-NC-4.0

from typing import List, Dict, Union
from tqdm import tqdm

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from mt_evaluation.models.base import Model
from mt_evaluation.core import FewShots


class Qwen3(Model):

    def __init__(self, model_id: str):
        super().__init__(model_id)

        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id, torch_dtype=torch.bfloat16, device_map="auto"
        ).eval()

        self.tokenizer.padding_side = "left"

    def __call__(
        self,
        system_prompts: List[str],
        user_prompts: List[str],
        few_shots: List[FewShots],
        batch_size: int = 4,
        enable_thinking: bool = False,
        **kwargs,
    ) -> List[str]:
        """
        Generate the answer to the given prompts

        Args:
            system_prompts (List[str]): List of system_prompts
            user_prompts (List[str]): List of user prompts, to be associated to system_prompts
            few_shots (List[Dict]): List of examples paired with the desired model answers to use for in-context learning
            batch_size (int): Number of prompts to process in one batch
            max_new_tokens (int): Maximum number of tokens to be generated by the model

        Returns:
            List[str]: List of responses, one per prompt
        """
        # TODO: currently, qwen3 is the only model that is using sampling (due to the recommended best practices)
        #   either use greedy decoding, or switch to sampling with the other models as well

        # TODO: currently, qwen3 does not manage to follow the format specified through the few shot samples
        #   it adds bold, and generates several error summaries.
        #   Probably we need to clarify the output format in the instruction, but this would modify the prompt.
        generation_kwargs = {}
        # From Qwen3 Best Practices @ https://huggingface.co/Qwen/Qwen3-8B#best-practices
        if enable_thinking:
            generation_kwargs["temperature"] = 0.6
            generation_kwargs["top_p"] = 0.8
            generation_kwargs["top_k"] = 20
            generation_kwargs["min_p"] = 0
            generation_kwargs["do_sample"] = True
            generation_kwargs["max_new_tokens"] = 32768
        else:
            generation_kwargs["temperature"] = 0.7
            generation_kwargs["top_p"] = 0.8
            generation_kwargs["top_k"] = 20
            generation_kwargs["min_p"] = 0
            generation_kwargs["do_sample"] = True
            generation_kwargs["max_new_tokens"] = 8192

        responses = []
        # Process sources in batches
        for i in tqdm(
            range(0, len(user_prompts), batch_size),
            desc=f"Running inference with {self.name}",
            total=(len(user_prompts) + batch_size - 1) // batch_size,
        ):
            batch_system_prompts = system_prompts[i : i + batch_size]
            batch_user_prompts = user_prompts[i : i + batch_size]
            batch_few_shots = few_shots[i : i + batch_size]

            messages = []
            for system_prompt, user_prompt, sample_few_shots in zip(
                batch_system_prompts, batch_user_prompts, batch_few_shots
            ):
                if enable_thinking:
                    assert (
                        len(sample_few_shots.user_prompts) == 0
                        and len(sample_few_shots.assistant_responses) == 0
                    ), "Cannot use in-context learning with thinking."

                sample_message = [
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                ]

                for shot_user_prompt, shot_response in zip(
                    sample_few_shots.user_prompts, sample_few_shots.assistant_responses
                ):
                    sample_message += [
                        {
                            "role": "user",
                            "content": shot_user_prompt,
                        },
                        {
                            "role": "assistant",
                            "content": shot_response,
                        },
                    ]

                sample_message += [
                    {
                        "role": "user",
                        "content": user_prompt,
                    },
                ]

                messages.append(sample_message)

            inputs = self.tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
                padding=True,
                enable_thinking=enable_thinking,
            ).to(self.model.device)

            with torch.inference_mode():
                outputs = self.model.generate(
                    **inputs,
                    **generation_kwargs,
                )

            outputs = outputs[:, inputs["input_ids"].shape[1] :].tolist()

            # parse thinking content
            decoded_thinking, decoded_outputs = [], []
            for sample_outputs in outputs:

                if self.tokenizer.convert_tokens_to_ids("</think>") in sample_outputs:
                    right_think_index = len(sample_outputs) - sample_outputs[
                        ::-1
                    ].index(self.tokenizer.convert_tokens_to_ids("</think>"))
                else:
                    right_think_index = 0

                # decoded thinking is currently unused
                decoded_thinking.append(
                    self.tokenizer.decode(
                        sample_outputs[:right_think_index],
                        skip_special_tokens=True,
                    )
                )

                decoded_outputs.append(
                    self.tokenizer.decode(
                        sample_outputs[right_think_index:],
                        skip_special_tokens=True,
                    )
                )

            responses += decoded_outputs

        return responses
